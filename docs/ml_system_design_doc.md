# ML System Design Doc
## Дизайн ML системы - Universal Document OCR PoC итерация 1

### 1. Цели и предпосылки 
#### 1.1. Зачем идем в разработку продукта?  

- Бизнес-цель: ускорение(конкурентное преимущество) и удешевление(уменьшение стоимости операционных процессов) процесса обработки сканов документов для заказчика.

- Почему станет лучше, чем сейчас, при использовании VLM по сравнению с более традиционными OCR DL подходами?

Использование систем OCR DL предполагает довольно тонкую настройку всего OCR пайплайна на шаблон имеющегося документа.
В организации очень много различных документов самых разных шаблонов от самых разных клиентов, заказчиков и т.д. Это вынуждает команду работающую с OCR DL разрабатывать все новые и новые узкоспециализированные пайпайлны обработки документов. 
Использование Visual LM/LLM для извлечения важных данных из сканов документов обеспечивает большую универсальность в их обработке.

- Что будем считать успехом итерации с точки зрения бизнеса?

**На 1-вой итерации нашего PoC:**
* Будет осуществлен бенчмарк для VLLM моделей, по обработке синтетических документов несуществующих граждан РФ (сейчас идет PoC исследование на синтетических данных, которые не являются настоящими документами).
* Сформирован отчет о протестированных моделях, достигнутых метриках, производительности моделей и потребляемых ресурсах.
#### 1.2. Бизнес-требования и ограничения  

- Краткое описание БТ
1. Модель должна уметь считывать с изображений информацию на русском и английском языках.
2. Модель должна обеспечивать сопоставимый уровень по корректности извлеченных данных с уже имеющимся у компании решением этой задачи.
* Уровень корректности извлечения данных оценивается по метрике `CER` ([ссылка](../cards/Метрики%20для%20оценки%20работы%20моделей.md)) для всех извлеченных полей данных на тестовом датасете, предоставленном Заказчиком.
* Целевой уровень метрики `CER` получаемый текущим решением компании находится под `NDA`.

- Бизнес-ограничения
1. Обработка одного документа не более 1 секунды
2. Модель VLLM должна умещаться на одну GPU

- Что мы ожидаем от конкретной итерации

На данной 1-вой итерации нашего PoC важно оценить корректность извлечения данных с использованием VLLM на синтетических данных с соблюдением бизнес ограничений.
Данная стадия посвящена оценке самой возможности использования VLLM в задаче извлечения данных с документов.
Интеграции MVP-сервиса в контур компании не предполагается. 

- Что считаем успешным пилотом? Критерии успеха и возможные пути развития проекта

* Модель должна выполнять бизнес требования заказчика.
* По возможности приближаться по своим параметрам к бизнес-ограничениям.

#### 1.3. Что входит в скоуп проекта/итерации, что не входит   

- На данной итерации PoC мы должны полностью выполнить бизнес требования заказчика:
1. Модель должна уметь считывать с изображений информацию на русском и английском языках

- Что не будет закрыто
1. Поскольку проект на стадии PoC, не гарантируется, что среди протестированных моделей будет найдена та, что сможет обеспечивать сопоставимый уровень по корректности извлеченных данных с уже имеющимся у компании решением.
2. Поскольку проект на стадии PoC, не обязательно выполнять Бизнес-ограничения полностью, но желательно к ним приближаться.

- Описание результата с точки зрения качества кода и воспроизводимости решения

**Воспроизводимость:**
1. `Git`-репозиторий с кодом бенчмарка модели
2. `Docker`-образ для полного воспроизведения рабочего окружения
3. Воспроизводимость пайплана тестирования моделей на датасете с помощью `DVC`

**Качество кода:**
1. Использовать авто форматирование кода на основе `Black` и `isort`
2. Использовать pre-commit hooks с `ruff` и `black` для обеспечения качества кода
3. Использовать `Google Style with type annotations` ([ссылка](../cards/Стили%20документации%20docstring%20в%20Python.md)) для python docstring

- Описание планируемого технического долга
1. Разработка сервиса с моделью VLLM к которому можно будет отправлять запросы по Fast API не входит в данную стадию работы над проектом.

#### 1.4. Предпосылки решения

1. Получить от компании датасет с синтетическими документами несуществующих граждан РФ (ожидаю)
2. Тестировать доступные открытые модели VLM на синтетических данных компании/открытых данных найденных в сети интернет (сейчас так).
3. Разработать бенчмарк оценки моделей на синтетическом датасете
4. Сделать выводы о соответствии бизнес требованиям и бизнес ограничениям

### 2. Методология 

#### 2.1. Постановка задачи  

С технической точки зрения, мы решаем задачу **Document Visual Question Answering** ([ссылка](../cards/Задачи%20решаемые%20моделями%20VLLM%20для%20документов.md)).

Обычно VLLM-модель при тестировании на открытых датасетах (например, DocVQA, RusTitW, TextVQAval, InstructDoc, ChartQA) отвечает на общие вопросы самой разной направленности.
Предугадать все возможное множество вопросов по этим данным заранее нельзя, и моделям дают "универсальный промпт" который описывает модели как ей лучше отвечать на вопросы из данного набора данных.

Напротив, при работе со сканом документа возникает более узкая задача извлечения информации по полям документа, например:
* Счет-фактура ИНН Покупателя
* Счет-фактура ИНН Продавца

Количество вариантов вопросов ограничено, - можно попробовать подобрать индивидуальный промпт на извлечение нужных данных для каждого поля каждого типа документа. Техники подбора этих промптов могут отличаться.
В случае если окажется, что индивидуальные промпты можно будет объединить в некоторую общую форму без потери уровня целевой метрики `CER` ([ссылка](../cards/Метрики%20для%20оценки%20работы%20моделей.md)), можно будет объединить их в system prompt.

**Методология:**
* подобрать наборы индивидуальных промптов для каждой тестируемой VLLM-модели для извлечения данных опр. поля для каждого типа документа
* оценить корректность всех извлеченных данных из полей документов по этим наборам промптов на тестовом датасете, предоставленном Заказчиком, по метрике `CER` ([ссылка](../cards/Метрики%20для%20оценки%20работы%20моделей.md)).
* выбрать лучшие промпты, дающие минимальный уровень по метрике `CER` ([ссылка](../cards/Метрики%20для%20оценки%20работы%20моделей.md)).
* проанализировать возможность подбора system promt по лучшим индивидуальным промптам.
* проанализировать полученные результаты.
1. модель должна обеспечивать сопоставимый уровень по корректности извлеченных данных с уже имеющимся у компании решением этой задачи.
2. целевой уровень метрики `CER` получаемый текущим решением компании находится под `NDA`.
3. принять решение о потенциале использования VLLM-модели в сервисах Заказчика для автоматической обработки документов. В частности возможности дообучения модели на данных заказчика с целью улучшения целевой метрики.

#### 2.2. Блок-схема PoC решения

![](../files/ml_system_design_doc-20241230.png)

#### 2.3. Этапы решения задачи `Data Scientist`

Процесс решения поставленной задачи будет происходить в несколько этапов:

- Этап 1 Анализ открытых данных и формирование требований к датасетам
- Этап 2 Тестирование VLLM моделей и отбор промптов для них
- Этап 3 Выбираем оптимальный набор промптов для каждой модели
- Этап 4 Оценка открытых VLLM моделей

##### Этап 1. Анализ открытых данных и формирование требований к датасетам

Мы хотим исследовать возможность использования Visual LM/LLM для извлечения важных данных из сканов документов граждан РФ:
* ИНН
* СНИЛС

Произведённый анализ показал, что подобные датасеты отсутствуют в открытом виде.
Открыты датасеты используемые для тестирования VLLM моделей (такие как, DocVQA, RusTitW, TextVQAval, InstructDoc, ChartQA) содержат в себе документы других типов.

На этапе поиска открытых датасетов были выявлены следующие сложности:
* Конфиденциальность - использование реальных сканов данных документов затруднительно поскольку они представляют собой конфиденциальную информацию граждан РФ
* Открытые датасеты не содержат в себе данных типов документов образцов используемых в РФ

Поэтому, после анализа доступных открытых данных было решено перейти к генерации сканов документов граждан РФ на основе пустых шаблонов документов и автоматизированном их заполнении с помощью генератора случайных чисел с использование данных об именах, фамилия и т.д. из библиотеки faker. Таким, образом оценка работы VLLM-моделей будет производится на датасетах с синтетическими документами несуществующих граждан РФ, которые были получены с помощью генератора случайных чисел.

Разработкой генератора синтетических документов занимается отдельная команда. Я использую в своей работе датасеты, которые они мне предоставляют.

Был разработан формат датасета ([ссылка на утвержденный формат](../cards/approved_dataset_format_for_vqa.md)).

* Описание данных/сущностей, есть ли выявленные проблемы с объемом/качеством/разметкой? Какие риски и проблемы были выявлены на этапе EDA?

Используется синтетический набор данных, который получается из специального генератора датасетов:
* может быть сгенерирован любой необходимый объем данных
* с разметкой данных проблем нет, - данные генерируются спец фреймворком. Если будут обнаружены проблемы с разметкой, - команда разрабатывающая фреймворк их генерации подправит код и будет получен новый корректный датасет
* с качеством данных проблем нет, - изначально документы генерируются в максимально высоком разрешении, далее может быть сформирован датасет в котором все синтетические изображения документов будут приведены к указанному пользователем разрешению.

VLLM модель хорошо извлекает информацию из документов высокого разрешения. С ростом разрешения растут и потребляемые ресурсы. Хотим подобрать минимальное разрешение на которых модель все еще выдает корректные данные, и при этом потребляем минимально количество ресурсов. Все сканы документов одного типа будут масштабироваться к этому размеру перед входом в модель. Это обеспечит батч инференс, оптимизирует используемые ресурсы и повысит производительность VLLM модели.

> Одним из параметров работы модели будет как раз такой подобранный размер документа.

**Риски:**
* Наличие существенной разницы между датасетам синтетических данных и настоящими документами

Настоящие сканы реальных документов могут отличаться от синтетических, например иметь различные артефакты: засветки, повреждения и т.д. Планируется симулировать их с помощью специальных аугментаций.

* Описание процесса генерации данных (откуда данные поступают, в каком формате, как выглядит этот процесс, регулярный ли он и т.п.)

Я получаю готовый датасет синтетических данных от команды, которая занимается их генерацией.
Датасет имеет указанный формат([ссылка на утвержденный формат](../cards/approved_dataset_format_for_vqa.md)).
При тестировании на датасете (например `SNILSes_x10`) происходит следующее.
Ниже показана `С4-диаграмма` уровня С3 для компонента "VLLM".

![](../files/ml_system_design_doc-20241230-1.png)

1. (на С1 уровне, не показано тут) `BenchmarkScheduler` - считывает пользовательский конфиг `user config`.

`user config` - это csv-файл, каждый ряд которого описывает процесс бенчмаркинга VLLM-модели на датасете.

| dataset     | framework    | model         | docker_image                                                                | system_promt       | prompt_collection | metrics                  | only_evaluate_metrics | metrics_aggregators                   |
| ----------- | ------------ | ------------- | --------------------------------------------------------------------------- | ------------------ | ----------------- | ------------------------ | --------------------- | ------------------------------------- |
| SNILSes_x10 | Hugging Face | Qwen2-VL-2B   | ghcr.io/vlmhyperbenchteam/qwen2-vl:ubuntu22.04-cu124-torch2.4.0_v0.1.0      | "sys_prompt_3.txt" | "Qwen2-VL-72.csv" | ""                       | Fasle                 | ""                                    |
| SNILSes_x10 | vLLM         | MiniCPM-V_2.6 | ghcr.io/vlmhyperbenchteam/minicpm-v_2.6:ubuntu22.04-cu124-torch2.4.0_v0.1.0 | "sys_prompt_5.txt" | "GPT4o.csv"       | '["WER", "CER", "BLEU"]' | Fasle                 | '["by_id", "by_doc_type", "overall"]' |

2. `BenchmarkScheduler` - запускает контейнер С3-диаграммы `VLLM`.

Этот программный компонент является `Docker-контейнером` внутри которого:
- настроено рабочее окружение для запуска на указанном в `user config` фреймворке инференса VLLM-моделей(например, `vLLM`), указанной модели (например, `MiniCPM-V_2.6`).
- не содержится сама VLLM-модель, она скачивается автоматически при первом запуске данного контейнера из репозитория фреймворка для инференса (например, `vLLM`).
- есть специальный класс-обвязка, который содержит метод для получения ответа от модели VLLM по данному изображению документа(image) на вопрос на естественным языке, который был задан по этому документу с целью извлечения необходимых данных (prompt).

Все происходящее внутри Docker-контейнера выделено пунктирной линией `VLLM Model Container`.

При запуске Docker-контейнера к нему монтируются папки:
* `SystemPrompts` - папка, внутри папки с названиями VLLM-моделей (например, `MiniCPM-V_2.6`). В каждой папке txt c system prompt (например, `sys_prompt_5.txt`)
* `PromptCollection` - 

* Внутри Docker-контейнера запускается класс `SystemPromptAdapter`, который считывает указанный  в `user config` файл `sys_prompt_5.txt` в кодировке `utf-8`, содержащий текстовый файл с текстом системного промпта для VLLM-модели на естественном языке.
* Внутри Docker-контейнера запускается класс-обвязка для VLLM-модели, который инициализирует модель.
* 


    - Описание процесса генерации данных (откуда данные поступают, в каком формате, как выглядит этот процесс, регулярный ли он и т.п.)
    - Если данных не достаточно, каковы способы решения этой проблемы, сколько данных еще нужно?
    - Есть ли в данных конфиденциальная информация? Нужно ли ее как-то обрабатывать?
    - Необходимый результат этапа
    - Любая полезная, по-вашему, информация на данном этапе...
## Этап 1 Тестирование VLLM моделей и  отбор промптов для них

1. **Модель-ответчик**: выбираем модель, которая будет отвечать на вопросы по картинкам.
* это все VLLM модели, которые мы планируем тестировать нашим бенчмарком ([ссылка](selected_models_for_benchmark_evaluation.md)).
* например, `Qwen2-VL-2B`.

2. **Модель-генератор промпта**: выбираем большую модель (`GPT4o`, `DeepSeek`, `Qwen2-VL-72B`), которая будет предлагать нам пропмт для **модели-ответчика**.

* Это любые Большие VLLM, доступные по API или через web-интерфейс.
* Выбрать 5-6 моделей-лидеров на сайте ([ссылка](https://lmarena.ai/)) и или любые другие представляющие интерес и работать с ними.

3. **Данные:** датасеты из этапа 1 ([ссылка](datasets_for_the_benchmark.md)).

### Отбор VLLM модели

* Прочитали про новую перспективную VLLM-модель
* Хотим предварительно оценить ее возможности

**Примерный порядок:**
* идем в веб-демо или API модели
* берем 1-3 картинки из датасета
* задаем ей вопросы
* смотрим ответы
* думаем стоит ли работать с этой моделью
Если она имеет некоторый потенциал, переходим к этапу подбора промптов для нее.

### Подбор промптов для VLLM модели

**Примерный порядок:**
* идем в веб-демо или API **модели-генератора промптов**
* пишем ей **наш промпт** с просьбой сгенерировать оптимальный промпт для **модели-ответчика**
Промпт на всякий случай сохраняем себе, возможно пригодится при работе с другими **моделями генераторами промптов**.
* получаем промпт для **модели-ответчика**
* переходим к **мини бенчмарку** оценивающему ответы **модели-ответчика** на датасетах этапа 1 ([ссылка](datasets_for_the_benchmark.md)).
* записываем в него тестируемый промпт, запускаем быстрый мини бенчмарк и смотрим полученные метрики
* если метрики имеют потенциал, сохраняем промпт.
* если нет, пробуем улучшить данный промпт
1. Можем написать новый промпт для **модели-генератора промптов**, указав ей полученные метрики на **мини бенчмарке** от предложенного ей ранее промпта. И попросить ее оптимизировать предложенный ей ранее промпт. Повторяем этот процесс циклически, смотрим улучшаются ли метрики на мини бенчмарке?
2. Использовать специальные инструменты для оптимизации промпта, например [textgrad](https://github.com/zou-group/textgrad) .
3. Применять любые техники и подходы

Записываем лучшие промпты в csv-файлы.
Название файла указываем в виде:
```
<название_модели_ответчика>_<название_модели_генератора_промпта>.csv
```

Например, мы получили "Qwen2-VL-2B_sep_GPT4o.csv", вида:

| doc type question type | optimal prompt                | cer |
| ---------------------- | ----------------------------- | --- |
| Паспорт Имя            | "Текст опт. промпта от GPT4o" | 0.1 |
| Паспорт Номер паспорта | "Текст опт. промпта от GPT4o" | 0.2 |
| СНИЛС                  | "Текст опт. промпта от GPT4o" | 0.2 |
И еще 5 подобных файлов от других **моделей-генераторов-промпта**.
* "Qwen2-VL-2B_sep_Qwen2-VL-72B.csv"
* "Qwen2-VL-2B_sep_DeepSeek.csv"
...
* и т.д.

## Этап 2 Выбираем оптимальный набор промптов для каждой модели

С этапа 1 для каждой тестируемой в бенчмарке модели ([ссылка](selected_models_for_benchmark_evaluation.md)) у нас есть набор промптов от различных **моделей-генераторов-промпта**:

* "Qwen2-VL-2B_sep_GPT4o.csv"
* "Qwen2-VL-2B_sep_Qwen2-VL-72B.csv"
* "Qwen2-VL-2B_sep_DeepSeek.csv"
...
* и т.д.

На этом этапе мы отбираем **лучшие промпты** из всех возможных вариантов для выбранной **VLLM-модели** на больших датасетах, сравнивая их на "хорошей" статистике.

**Данные:** датасеты из этапа 2 ([ссылка](datasets_for_the_benchmark.md)).

На этом этапе бенчмарка оцениваем:
* имеющиеся наборы промптов (от **моделей-генераторов промптов**) для извлечения нужной информации из документов для каждой тестируемой в бенчмарке модели ([ссылка](selected_models_for_benchmark_evaluation.md))
* оцениваем по метрикам корректность извлекаемых данных
* отбираем лучшие промпты для каждой модели ([ссылка](selected_models_for_benchmark_evaluation.md)) из всех представленных вариантов

Мы хотим, чтобы у **VLLM-модели** не было трудностей с чтением информации с документа.

## Этап 3 Исследуем работу VLLM-модели на всевозможных данных

На этом этапе исследуем работу **VLLM-модели** с одним оптимально подобранным для нее набором промптов(на этапе 2) на максимально разнообразных данных.

Исследуем возможности **VLLM-модели**, знакомим ее со всеми возможными данными, создаем ей различные трудности: разрешение, аугментации и т.д.

**Данные:** датасеты из этапа 3 ([ссылка](datasets_for_the_benchmark.md)).

На этом этапе бенчмарка оцениваем:
* работу **VLLM-модели** ([ссылка](selected_models_for_benchmark_evaluation.md)) на максимально разнообразных данных
* исследуем туда ли смотрит модель?
* правильно ли распознается текст и каких ситуациях?
* ищем слабые места и точки роста
* обсуждаем со стейкхолдером имеющиеся точки роста и пути их достижения


1. Описать "Этап 1. Подготовка данных" - **2 балла**:
    
    - Описание данных/сущностей, есть ли выявленные проблемы с объемом/качеством/разметкой? Какие риски и проблемы были выявлены на этапе EDA?
    - Описание процесса генерации данных (откуда данные поступают, в каком формате, как выглядит этот процесс, регулярный ли он и т.п.)
    - Если данных не достаточно, каковы способы решения этой проблемы, сколько данных еще нужно?
    - Есть ли в данных конфиденциальная информация? Нужно ли ее как-то обрабатывать?
    - Необходимый результат этапа
    - Любая полезная, по-вашему, информация на данном этапе...
2. Описать "Этап 2. Подготовка прогнозных моделей" - **2 балла**:
    
    - Описание ML-метрик и функций потерь, выбранные для решения задачи с обоснованием этого выбора
    - Описание схемы ML-валидации, с учетом специфики состава данных, бизнес-задачи, функциональных и нефункциональных требований
    - Описание структуры бейзлайна (одного или нескольких), предобработки и процесса моделирования для бейзлайна
    - Стратегии дальнейшего развития решения: выбор моделей, предобработка, FE, оптимизация гиперпараметров т.п.
    - Анализ и интерпретация работы модели
    - Риски данного этапа, и способы их снижения
    - Необходимый результат этапа
    - Любая полезная, по-вашему, информация на данном этапе...
    
**NB**: список этапов можно и нужно расширить/реструктурировать под вашу задачу.

- Для каждого этапа **по результатам EDA** описываем - **отдельно для бейзлайна** и **отдельно для основного MVP** - все про данные и технику решения максимально конкретно. Обозначаем необходимые вводные, технику предполагаемого решения и что ожидаем получить на выходе, чтобы перейти к следующему этапу.
- Как правило, детальное и структурированное заполнение раздела `2.3` возможно только **по результатам EDA**.
- Если описание в дизайн доке **шаблонно** - т.е. его можно скопировать и применить к разным продуктам, то оно **некорректно**. Дизайн док должен показывать схему решения для конкретной задачи, поставленной в части 1.

> Примеры этапов:
> 
> - Этап 2 - Подготовка прогнозных моделей
> - Этап 3 - Интерпретация моделей (согл. с заказчиком)
> - Этап 4 - Интеграция бизнес правил для расчета бизнес-метрик качества модели
> - Этап 5 - Подготовка инференса модели по итерациям
> - Этап 6 - Интеграция бизнес правил
> - Этап 7 - Разработка оптимизатора (выбор оптимальной итерации)
> - Этап 8 - Подготовка финального отчета для бизнеса

_Этап 1 - это обычно, подготовка данных._

В этом этапе должно быть следующее:

- Данные и сущности, на которых будет обучаться ваша модель машинного обучения. Отдельная таблица для целевой переменной (либо целевых переменных разных этапов), отдельная таблица – для признаков.

|Название данных|Есть ли данные в компании (если да, название источника/витрин)|Требуемый ресурс для получения данных (какие роли нужны)|Проверено ли качество данных (да, нет)|
|---|---|---|---|
|Продажи|DATAMARTS_SALES_PER_DAY|DE/DS|+|
|...|...|...|...|

- Краткое описание результата этапа - что должно быть на выходе: витрины данных, потоки данных, др.

> Чаще всего заполнение раздела невозможно без EDA.

_Этапы 2 и далее, помимо подготовки данных._

Описание техники **для каждого этапа** должно включать описание **отдельно для MVP** и **отдельно для бейзлайна**:

- Описание формирования выборки для обучения, тестирования и валидации. Выбор репрезентативных данных для экспериментов, обучения и подготовки пилота (от бизнес-цели и репрезентативности данных с технической точки зрения) `Data Scientist`
- Горизонт, гранулярность, частоту необходимого пересчета прогнозных моделей `Data Scientist`
- Определение целевой переменной, согласованное с бизнесом `Data Scientist`
- Какие метрики качества используем и почему они связаны с бизнес-результатом, обозначенным `Product Owner` в разделах `1` и `3`. Пример - WAPE <= 50% для > 80% категорий, bias ~ 0. Возможна формулировка в терминах относительно бейзлайна, количественно. Для бейзлайна могут быть свои целевые метрики, а может их вообще не быть (если это обосновано) `Data Scientist`
- Необходимый результат этапа. Например, необходимым результатом может быть не просто достижение каких-либо метрик качества, а включение в модели определенных факторов (флаг промо для прогноза выручки, др.) `Data Scientist`
- Какие могут быть риски и что планируем с этим делать. Например, необходимый для модели фактор (флаг промо) окажется незначимым для большинства моделей. Или для 50% моделей будет недостаточно данных для оценки `Data Scientist`
- Верхнеуровневые принципы и обоснования для: feature engineering, подбора алгоритма решения, техники кросс-валидации, интерпретации результата (если применимо).
- Предусмотрена ли бизнес-проверка результата этапа и как будет проводиться `Data Scientist` & `Product Owner`