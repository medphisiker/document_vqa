# ML System Design Doc
## Дизайн ML системы - Universal Document OCR PoC итерация 1

### 1. Цели и предпосылки 
#### 1.1. Зачем идем в разработку продукта?  

- Бизнес-цель: ускорение(конкурентное преимущество) и удешевление(уменьшение стоимости операционных процессов) процесса обработки сканов документов для заказчика.

- Почему станет лучше, чем сейчас, при использовании VLM по сравнению с более традиционными OCR DL подходами?

Использование систем OCR DL предполагает довольно тонкую настройку всего OCR пайплайна на шаблон имеющегося документа.
В организации очень много различных документов самых разных шаблонов от самых разных клиентов, заказчиков и т.д. Это вынуждает команду работающую с OCR DL разрабатывать все новые и новые узкоспециализированные пайпайлны обработки документов. 
Использование Visual LM/LLM для извлечения важных данных из сканов документов обеспечивает большую универсальность в их обработке.

- Что будем считать успехом итерации с точки зрения бизнеса?

**На 1-вой итерации нашего PoC:**
* Будет осуществлен бенчмарк для VLLM моделей, по обработке синтетических документов несуществующих граждан РФ (сейчас идет PoC исследование на синтетических данных, которые не являются настоящими документами).
* Сформирован отчет о протестированных моделях, достигнутых метриках, производительности моделей и потребляемых ресурсах.
#### 1.2. Бизнес-требования и ограничения  

- Краткое описание БТ
1. Модель должна уметь считывать с изображений информацию на русском и английском языках.
2. Модель должна обеспечивать сопоставимый уровень по корректности извлеченных данных с уже имеющимся у компании решением этой задачи.
* Уровень корректности извлечения данных оценивается по метрике `CER` ([ссылка](../cards/Метрики%20для%20оценки%20работы%20моделей.md)) для всех извлеченных полей данных на тестовом датасете, предоставленном Заказчиком.
* Целевой уровень метрики `CER` получаемый текущим решением компании находится под `NDA`.

- Бизнес-ограничения
1. Обработка одного документа не более 1 секунды
2. Модель VLLM должна умещаться на одну GPU

- Что мы ожидаем от конкретной итерации

На данной 1-вой итерации нашего PoC важно оценить корректность извлечения данных с использованием VLLM на синтетических данных с соблюдением бизнес ограничений.
Данная стадия посвящена оценке самой возможности использования VLLM в задаче извлечения данных с документов.
Интеграции MVP-сервиса в контур компании не предполагается. 

- Что считаем успешным пилотом? Критерии успеха и возможные пути развития проекта

* Модель должна выполнять бизнес требования заказчика.
* По возможности приближаться по своим параметрам к бизнес-ограничениям.

#### 1.3. Что входит в скоуп проекта/итерации, что не входит   

- На данной итерации PoC мы должны полностью выполнить бизнес требования заказчика:
1. Модель должна уметь считывать с изображений информацию на русском и английском языках

- Что не будет закрыто
1. Поскольку проект на стадии PoC, не гарантируется, что среди протестированных моделей будет найдена та, что сможет обеспечивать сопоставимый уровень по корректности извлеченных данных с уже имеющимся у компании решением.
2. Поскольку проект на стадии PoC, не обязательно выполнять Бизнес-ограничения полностью, но желательно к ним приближаться.

- Описание результата с точки зрения качества кода и воспроизводимости решения

**Воспроизводимость:**
1. `Git`-репозиторий с кодом бенчмарка модели
2. `Docker`-образ для полного воспроизведения рабочего окружения
3. Воспроизводимость пайплана тестирования моделей на датасете с помощью `DVC`

**Качество кода:**
1. Использовать авто форматирование кода на основе `Black` и `isort`
2. Использовать pre-commit hooks с `ruff` и `black` для обеспечения качества кода
3. Использовать `Google Style with type annotations` ([ссылка](../cards/Стили%20документации%20docstring%20в%20Python.md)) для python docstring

- Описание планируемого технического долга
1. Разработка сервиса с моделью VLLM к которому можно будет отправлять запросы по Fast API не входит в данную стадию работы над проектом.

#### 1.4. Предпосылки решения

1. Получить от компании датасет с синтетическими документами несуществующих граждан РФ (ожидаю)
2. Тестировать доступные открытые модели VLM на синтетических данных компании/открытых данных найденных в сети интернет (сейчас так).
3. Разработать бенчмарк оценки моделей на синтетическом датасете
4. Сделать выводы о соответствии бизнес требованиям и бизнес ограничениям

### 2. Методология 

#### 2.1. Постановка задачи  

С технической точки зрения, мы решаем задачу **Document Visual Question Answering** ([ссылка](../cards/Задачи%20решаемые%20моделями%20VLLM%20для%20документов.md)).

Обычно VLLM-модель при тестировании на открытых датасетах (например, DocVQA, RusTitW, TextVQAval, InstructDoc, ChartQA) отвечает на общие вопросы самой разной направленности.
Предугадать все возможное множество вопросов по этим данным заранее нельзя, и моделям дают "универсальный промпт" который описывает модели как ей лучше отвечать на вопросы из данного набора данных.

Напротив, при работе со сканом документа возникает более узкая задача извлечения информации по полям документа, например:
* Счет-фактура ИНН Покупателя
* Счет-фактура ИНН Продавца

Количество вариантов вопросов ограничено, - можно попробовать подобрать индивидуальный промпт на извлечение нужных данных для каждого поля каждого типа документа. Техники подбора этих промптов могут отличаться.
В случае если окажется, что индивидуальные промпты можно будет объединить в некоторую общую форму без потери уровня целевой метрики `CER` ([ссылка](../cards/Метрики%20для%20оценки%20работы%20моделей.md)), можно будет объединить их в system prompt.

**Методология:**
* подобрать наборы индивидуальных промптов для каждой тестируемой VLLM-модели для извлечения данных опр. поля для каждого типа документа
* оценить корректность всех извлеченных данных из полей документов по этим наборам промптов на тестовом датасете, предоставленном Заказчиком, по метрике `CER` ([ссылка](../cards/Метрики%20для%20оценки%20работы%20моделей.md)).
* выбрать лучшие промпты, дающие минимальный уровень по метрике `CER` ([ссылка](../cards/Метрики%20для%20оценки%20работы%20моделей.md)).
* проанализировать возможность подбора system promt по лучшим индивидуальным промптам.
* проанализировать полученные результаты.
1. модель должна обеспечивать сопоставимый уровень по корректности извлеченных данных с уже имеющимся у компании решением этой задачи.
2. целевой уровень метрики `CER` получаемый текущим решением компании находится под `NDA`.
3. принять решение о потенциале использования VLLM-модели в сервисах Заказчика для автоматической обработки документов. В частности возможности дообучения модели на данных заказчика с целью улучшения целевой метрики.

#### 2.2. Блок-схема PoC решения

![](../files/ml_system_design_doc-20241230.png)

#### 2.3. Этапы решения задачи `Data Scientist`

Процесс решения поставленной задачи будет происходить в несколько этапов:

- Этап 1 Анализ открытых данных и формирование требований к датасетам
- Этап 2 Тестирование VLLM моделей и отбор промптов для них
- Этап 3 Выбираем оптимальный набор промптов для каждой модели
- Этап 4 Оценка открытых VLLM моделей

##### Этап 1. Анализ открытых данных и формирование требований к датасетам

Мы хотим исследовать возможность использования Visual LM/LLM для извлечения важных данных из сканов документов граждан РФ:
* ИНН
* СНИЛС

Произведённый анализ показал, что подобные датасеты отсутствуют в открытом виде.
Открыты датасеты используемые для тестирования VLLM моделей (такие как, DocVQA, RusTitW, TextVQAval, InstructDoc, ChartQA) содержат в себе документы других типов.

На этапе поиска открытых датасетов были выявлены следующие сложности:
* Конфиденциальность - использование реальных сканов данных документов затруднительно поскольку они представляют собой конфиденциальную информацию граждан РФ
* Открытые датасеты не содержат в себе данных типов документов образцов используемых в РФ

Поэтому, после анализа доступных открытых данных было решено перейти к генерации сканов документов граждан РФ на основе пустых шаблонов документов и автоматизированном их заполнении с помощью генератора случайных чисел с использование данных об именах, фамилия и т.д. из библиотеки faker. Таким, образом оценка работы VLLM-моделей будет производится на датасетах с синтетическими документами несуществующих граждан РФ, которые были получены с помощью генератора случайных чисел.

Разработкой генератора синтетических документов занимается отдельная команда. Я использую в своей работе датасеты, которые они мне предоставляют.

Был разработан формат датасета ([ссылка на утвержденный формат](../cards/approved_dataset_format_for_vqa.md)).

* Описание данных/сущностей, есть ли выявленные проблемы с объемом/качеством/разметкой? Какие риски и проблемы были выявлены на этапе EDA?

Используется синтетический набор данных, который получается из специального генератора датасетов:
* может быть сгенерирован любой необходимый объем данных
* с разметкой данных проблем нет, - данные генерируются спец фреймворком. Если будут обнаружены проблемы с разметкой, - команда разрабатывающая фреймворк их генерации подправит код и будет получен новый корректный датасет
* с качеством данных проблем нет, - изначально документы генерируются в максимально высоком разрешении, далее может быть сформирован датасет в котором все синтетические изображения документов будут приведены к указанному пользователем разрешению.

VLLM модель хорошо извлекает информацию из документов высокого разрешения. С ростом разрешения растут и потребляемые ресурсы. Хотим подобрать минимальное разрешение на которых модель все еще выдает корректные данные, и при этом потребляем минимально количество ресурсов. Все сканы документов одного типа будут масштабироваться к этому размеру перед входом в модель. Это обеспечит батч инференс, оптимизирует используемые ресурсы и повысит производительность VLLM модели.

> Одним из параметров работы модели будет как раз такой подобранный размер документа.

**Риски:**
* Наличие существенной разницы между датасетам синтетических данных и настоящими документами

Настоящие сканы реальных документов могут отличаться от синтетических, например иметь различные артефакты: засветки, повреждения и т.д. Планируется симулировать их с помощью специальных аугментаций.

* Описание процесса генерации данных (откуда данные поступают, в каком формате, как выглядит этот процесс, регулярный ли он и т.п.)

Я получаю готовый датасет синтетических данных от команды, которая занимается их генерацией.
Датасет имеет указанный формат([ссылка на утвержденный формат](../cards/approved_dataset_format_for_vqa.md)).
При тестировании на датасете (например `SNILSes_x10`) происходит следующее.
Ниже показана `С4-диаграмма` уровня С3 для компонента "VLLM".

![](../files/ml_system_design_doc-20241230-1.png)

1. (на С1 уровне, не показано тут) `BenchmarkScheduler`(контейнер С3 уровня) - считывает пользовательский конфиг `user config`.

`user config` - это csv-файл, каждый ряд которого описывает процесс бенчмаркинга VLLM-модели на датасете.

| dataset     | framework    | model         | docker_image                                                                | system_promt       | prompt_collection | metrics                  | only_evaluate_metrics | metrics_aggregators                   |
| ----------- | ------------ | ------------- | --------------------------------------------------------------------------- | ------------------ | ----------------- | ------------------------ | --------------------- | ------------------------------------- |
| SNILSes_x10 | Hugging Face | Qwen2-VL-2B   | ghcr.io/vlmhyperbenchteam/qwen2-vl:ubuntu22.04-cu124-torch2.4.0_v0.1.0      | "sys_prompt_3.txt" | "Qwen2-VL-72.csv" | ""                       | Fasle                 | ""                                    |
| SNILSes_x10 | vLLM         | MiniCPM-V_2.6 | ghcr.io/vlmhyperbenchteam/minicpm-v_2.6:ubuntu22.04-cu124-torch2.4.0_v0.1.0 | "sys_prompt_5.txt" | "GPT4o.csv"       | '["WER", "CER", "BLEU"]' | Fasle                 | '["by_id", "by_doc_type", "overall"]' |

2. `BenchmarkScheduler` - запускает `VLLM`(контейнер С3-диаграммы).

Этот программный компонент является `Docker-контейнером` внутри которого:
- настроено рабочее окружение для запуска на указанном в `user config` фреймворке инференса VLLM-моделей(например, `vLLM`), указанной модели (например, `Qwen2-VL-2B`).
- не содержатся веса VLLM-модели, они скачивается автоматически при первом запуске данного контейнера из репозитория фреймворка для инференса (например, `Hugging Face`).
- есть специальный класс-обвязка, который содержит метод для получения ответа от модели VLLM по данному изображению документа(`image`) на вопрос на естественным языке, который был задан по этому документу с целью извлечения необходимых данных (`prompt`).

Все происходящее внутри Docker-контейнера выделено пунктирной линией `VLLM Model Container`.

При запуске Docker-контейнера к нему монтируются папки:
* `SystemPrompts` - папка внутри нее txt-файлы в кодировке `utf-8` (например, `sys_prompt_3.txt`), каждый из которых содержит внутри один системный промпт на естественном языке.

Системные промпты расположены по пути:
```
./SystemPrompts/{DATASETNAME}/{MODELNAME}/sys_prompt_3.txt
```
где
{DATASETNAME} - название датасета (например `SNILSes_x10`)
{MODELNAME} - название VLLM-модели (например `Qwen2-VL-2B`)

* `PromptCollection` - папка внутри нее csv-файлы с коллекциями промптов в кодировке `utf-8` (например, `Qwen2-VL-72.csv`).

Каждая коллекция, например `Qwen2-VL-72.csv`, имеет следующее содержание:

| doc_type_question_type | optimal_prompt                 |
| ---------------------- | ------------------------------ |
| СНИЛС Имя              | "Текст промпта от Qwen2-VL-72" |
| СНИЛС Фамилия          | "Текст промпта от Qwen2-VL-72" |
| СНИЛС Номер            | "Текст промпта от Qwen2-VL-72" |
Коллекции промптов расположены по пути:
```
./PromptCollection/{DATASETNAME}/{MODELNAME}/Qwen2-VL-72.csv
```
где
{DATASETNAME} - название датасета (например `SNILSes_x10`)
{MODELNAME} - название VLLM-модели (например `Qwen2-VL-2B`)

* `Datasets` - папка c датасетами формата ([ссылка на утвержденный формат](../cards/approved_dataset_format_for_vqa.md)).
* `ModelsAnswers` - папка в которой храним csv с ответами VLLM моделей.

Ответы от тестируемой модели расположены в пути:
```
./ModelsAnswers/{DATASETNAME}_{FRAMEWORKNAME}_{MODELNAME}_{SYSTEMPROMPTFILENAME}.csv
```
где
{DATASETNAME} - название датасета (например, `SNILSes_x10`)
{FRAMEWORKNAME} - название фреймворка (например, `Hugging Face`)
{MODELNAME} - название VLLM-модели (например `Qwen2-VL-2B`)
{SYSTEMPROMPTFILENAME} - название текстового файла, содержащего системный промпт для модели (например, `sys_prompt_3)

3. Внутри Docker-контейнера запускается `SystemPromptAdapter`(компонент С3 уровня), который считывает указанный в `user config` txt-файл (например, `sys_prompt_3.txt`) в кодировке `utf-8`, содержащий текст системного промпта для VLLM-модели на естественном языке.
4. Внутри Docker-контейнера запускается класс-обвязка `VLLM`(компонент С3 уровня) для VLLM-модели(например `Qwen2-VL-2B`), который инициализирует модель и передает ей системный промпт от `SystemPromptAdapter`.
5. Внутри Docker-контейнера запускается `PromptAdapter`(компонент С3 уровня), который считывает указанный в `user config` файл (например, `Qwen2-VL-72.csv`) в кодировке `utf-8`, содержащий коллекцию промптов для VLLM-модели на естественном языке.
Внутри себя `PromptAdapter` сформирует хеш таблицу `key:doc_type_question_type` и `value:optimal_prompt`.
На запрос по полю `СНИЛС Имя` он вернет `Текст промпта от Qwen2-VL-72`.

6. Запускается `DatasetIteratorTask`(компонент С3 уровня), который считывает указанный в `user config` датасет (например, `SNILSes_x10`).
7.  `DatasetIteratorTask` итерируется в цикле по всем элементам датасета:
7.1 на каждой итерации цикла `DatasetIteratorTask` получает из датасета :
* `image` - изображение документа
* `doc_type_question_type` - ключ для получения промпта от `PromptAdapter`
7.2 на каждой итерации цикла `PromptAdapter` получает `doc_type_question_type` и отдает по нему соответствующий `prompt` для `VLLM`-компонента в виде модели.
7.3 на каждой итерации цикла `VLLM`-модель
получает:
* `image` - изображение документа 
* `prompt` - вопрос на естественным языке, который был задан по этому документу с целью извлечения необходимых данных (например, "Какая фамилия у гражданина?")
возвращает:
* `answer` - текстовый ответ (например, "Иванов")
7.4 на каждой итерации цикла полученный от `VLLM`-модели ответ `answer` передается `DatasetIteratorTask`, который будет хранить их.
8. Итерация по датасету заканчивается. `DatasetIteratorTask` записывает все ответы от модели в выходной csv-файл.

Данный выходной csv-файл с ответами от модели используется в дальнейшем для расчета метрики CER и анализа работы модели.

* Есть ли в данных конфиденциальная информация? Нужно ли ее как-то обрабатывать?

Мы используем датасеты с синтетическими документами несуществующих граждан РФ, которые были получены с помощью генератора случайных чисел.

* Необходимый результат этапа

1. Разработка всех необходимых модулей для бенчмарка
2. Получение от команды генерирующей данные датасетов:
* мини датасеты для `Этапа 2 Тестирования VLLM моделей и отбора промптов для них`
- крупные датасеты для `Этапа 3 Выбира оптимального набора промптов для каждой модели`
- крупные датасеты с аугментациями для `Этапа 4 Оценки открытых VLLM моделей`

для документов:
* ИНН
* СНИЛС
##### Этап 2 Тестирование VLLM моделей и отбор промптов для них

1. **Модель-ответчик**: выбираем модель, которая будет отвечать на вопросы по картинкам.
* это все VLLM модели, которые мы планируем тестировать нашим бенчмарком.
* например, `Qwen2-VL-2B`.

2. **Модель-генератор промпта**: выбираем большую языковую модель (`GPT4o`, `DeepSeek`, `Qwen2-VL-72B`), которая будет предлагать нам пропмт для **модели-ответчика**.
* это любые Большие VLLM, доступные по API или через web-интерфейс.
* выбрать 5-6 моделей-лидеров на сайте ([ссылка](https://lmarena.ai/)) и или любые другие представляющие интерес и работать с ними.

3. **Данные:** датасеты из этапа 1 ([ссылка](datasets_for_the_benchmark.md)).

###### Отбор VLLM модели

* Прочитали про новую перспективную VLLM-модель
* Хотим предварительно оценить ее возможности

**Примерный порядок работы:**
* идем в веб-демо или API модели
* берем 1-3 картинки из датасета
* задаем ей вопросы
* смотрим ответы
* думаем стоит ли брать эту модель в бенчмарк?
Если она имеет некоторый потенциал, переходим к этапу подбора промптов для нее.

###### Подбор промптов для VLLM модели

**Примерный порядок:**
* идем в веб-демо или API **модели-генератора промптов**
* пишем ей **наш промпт** с просьбой сгенерировать оптимальный промпт для **модели-ответчика**

Промпт на всякий случай сохраняем себе, возможно пригодится при работе с другими **моделями генераторами промптов**.

* получаем промпт для **модели-ответчика**
* переходим к **мини бенчмарку** оценивающему ответы **модели-ответчика** на датасетах этапа 1 ([ссылка](datasets_for_the_benchmark.md)).
* записываем в него тестируемый промпт, запускаем быстрый мини бенчмарк и смотрим полученную метрику CER.
* если метрика превышает 0.7, сохраняем промпт.
* если нет, пробуем улучшить данный промпт
1. Можем написать новый промпт для **модели-генератора промптов**, указав ей полученные метрики на **мини бенчмарке** от предложенного ей ранее промпта. И попросить ее оптимизировать предложенный ей ранее промпт. Повторяем этот процесс циклически, смотрим улучшаются ли метрики на мини бенчмарке?
2. Использовать специальные инструменты для оптимизации промпта, например [textgrad](https://github.com/zou-group/textgrad) .
3. Применять любые техники и подходы

Записываем лучшие промпты в csv-файлы коллекций промптов.
Получаем еще 5 подобных файлов от других **моделей-генераторов-промпта**.
* "Qwen2-VL-72B.csv"
* "DeepSeek.csv"
* "HumanPromptEnginner"
...
* и т.д.

##### Этап 3 Выбираем оптимальный набор промптов для каждой модели

С этапа 2 для каждой тестируемой в бенчмарке модели у нас есть набор промптов от различных **моделей-генераторов-промпта**:

* "Qwen2-VL-72B.csv"
* "DeepSeek.csv"
* "HumanPromptEnginner"
...
* и т.д.

На этом этапе мы отбираем **лучшие промпты** из всех возможных вариантов для выбранной **VLLM-модели** на больших датасетах, сравнивая их на "хорошей" статистике.

**Данные:** датасеты из этапа 2 ([ссылка](datasets_for_the_benchmark.md)).

На этом этапе бенчмарка оцениваем:
* имеющиеся наборы промптов (от **моделей-генераторов промптов**) для извлечения нужной информации из документов для каждой тестируемой в бенчмарке модели.
* оцениваем по метрике `CER` корректность извлекаемых данных
* отбираем лучшие промпты для каждой модели из всех представленных вариантов

Мы хотим, чтобы у **VLLM-модели** не было трудностей с чтением информации с документа.

##### Этап 4 Исследуем работу VLLM-модели на всевозможных данных

На этом этапе исследуем работу **VLLM-модели** с одним оптимально подобранным для нее набором промптов(на этапе 3) на максимально разнообразных данных.

Исследуем возможности **VLLM-модели**, знакомим ее со всеми возможными данными, создаем ей различные трудности: разрешение, аугментации и т.д.

**Данные:** датасеты из этапа 3 ([ссылка](datasets_for_the_benchmark.md)).

**На этом этапе бенчмарка оцениваем:**
* работу **VLLM-модели** на максимально разнообразных данных
* корректность извлечения данных по метрике CER. Должна превышать целевой уровень метрики от Заказчика
* формируем понимание текущих возможностей доступных VLLM-моделей и потенциальных точек роста для дообучения

**Необходимый результат этапа:**
* пишем отчет о проделанной работе для Заказчика.
* обсуждаем с заказчиком имеющиеся точки роста, пути их достижения, определяем возможность продолжения проекта на следующий этап его развития.
* если будет согласован следующий этап, делаем примерную оценку по возможным срокам работы и необходимым вычислительным ресурсам.
